{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Setup Project Environment and Dependencies",
        "description": "Initialize the PE Scanner Python project with the specified dependencies and project structure.",
        "details": "Create a virtual environment using Python 3.11+ for best compatibility. Install required libraries with specified minimum versions: pandas>=2.0.0, numpy>=1.24.0, yfinance>=0.2.28, pydantic>=2.0.0, tabulate>=0.9.0, rich>=13.0.0, pytest>=7.4.0, pytest-cov>=4.1.0. Set up the directory structure as per PRD, including src/pe_scanner with submodules, tests/, scripts/, portfolios/, outputs/, and .taskmaster/. Use pyproject.toml and requirements.txt for dependency management. Configure environment variables for API keys and caching.",
        "testStrategy": "Verify environment setup by running a simple script importing all dependencies. Run pytest to confirm test framework is operational. Check directory structure matches PRD specification.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Implement Yahoo Finance Data Fetcher Module",
        "description": "Develop the data fetching module to retrieve market data from Yahoo Finance using yfinance library.",
        "details": "Use yfinance>=0.2.28 to fetch current price, trailing P/E (TTM), forward P/E (FY1), trailing EPS (TTM), forward EPS (FY1), market cap, and last updated timestamp for given tickers. Implement caching with TTL (default 3600 seconds) to reduce API calls. Handle API rate limiting and errors gracefully. Validate data completeness and format using pydantic models.",
        "testStrategy": "Unit test fetching data for known tickers (e.g., HOOD, BATS.L). Simulate API failures and verify error handling. Confirm caching reduces repeated calls within TTL.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Develop Portfolio Loader for CSV/JSON Files",
        "description": "Create module to load portfolio data from CSV and JSON files for ISA, SIPP, and Wishlist portfolios.",
        "details": "Implement parsers for CSV and JSON formats matching portfolio schema (ticker, shares, cost_basis, current_price). Validate data using pydantic schemas to ensure correctness and completeness. Support loading multiple portfolios and merging if needed. Handle missing or malformed data with clear error messages.",
        "testStrategy": "Test loading sample portfolios (isa.csv, sipp.csv, wishlist.csv). Validate error handling for missing fields and invalid formats. Confirm data matches expected structure.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Implement P/E Compression Calculation Module",
        "description": "Build the core logic to calculate P/E compression percentage and interpret signals based on thresholds.",
        "details": "Implement formula: compression_pct = ((trailing_pe - forward_pe) / trailing_pe) * 100. Define thresholds: ±20% triggers detailed analysis, with higher thresholds for high and extreme compression. Classify compression as positive (undervalued) or negative (overvalued). Return compression value and signal classification.",
        "testStrategy": "Unit tests with various trailing and forward P/E values including edge cases (zero, negative, extreme values). Verify correct signal classification and threshold triggering.",
        "priority": "high",
        "dependencies": [
          2,
          3
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Implement UK Stock Data Correction Logic",
        "description": "Detect UK stocks by '.L' suffix and apply 100x correction to forward P/E and EPS if forward P/E < 1.0 to fix pence-to-pounds conversion errors.",
        "details": "In corrector.py, implement detection of UK stocks by ticker suffix '.L'. If forward P/E < 1.0, multiply forward EPS and forward P/E by 100. Validate corrections against other data points to avoid false positives. Log corrections applied for audit.",
        "testStrategy": "Test with known UK stocks (BATS.L, BAB.L, BT-A.L, RR.L) to confirm correction applied. Test non-UK stocks to ensure no correction. Verify no correction if forward P/E >= 1.0.",
        "priority": "high",
        "dependencies": [
          2,
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Implement Stock Split Detection Algorithm",
        "description": "Detect stock splits by analyzing forward EPS growth and cross-referencing known split dates to flag suspicious data.",
        "details": "Calculate implied EPS growth: (forward_eps - trailing_eps) / trailing_eps. If growth > 100%, flag for manual verification. Cross-reference with a maintained list of known stock split dates (can be static or fetched). Mark flagged stocks with data quality warnings. Provide interface for manual verification.",
        "testStrategy": "Test detection with NFLX example (known stock split error). Test with stocks without splits to confirm no false flags. Verify warnings appear correctly.",
        "priority": "high",
        "dependencies": [
          2,
          4,
          5
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Implement Data Quality Validation and Flagging",
        "description": "Develop comprehensive data quality checks including extreme downside projections, missing data, and stale analyst estimates.",
        "details": "Check for extreme downside projections (-95% to -100%) and flag as unreliable. Detect missing forward P/E or EPS data and mark accordingly. Validate analyst estimate timestamps; flag if older than 6 months. Use pydantic for schema validation and custom validators for business rules. Aggregate flags for reporting.",
        "testStrategy": "Unit tests with synthetic data covering all validation rules. Confirm flags trigger correctly and no false positives. Integration test with data fetcher and corrector modules.",
        "priority": "high",
        "dependencies": [
          2,
          5,
          6
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Implement Fair Value Scenario Calculations (Bear and Bull Cases)",
        "description": "Calculate bear and bull case fair values and upside percentages based on forward EPS and fixed P/E multiples.",
        "details": "Implement formulas: bear_fair_value = forward_eps × 17.5, bull_fair_value = forward_eps × 37.5. Calculate upside percentages relative to current price. Make P/E multiples configurable via config.yaml. Return values for use in reports and signal generation.",
        "testStrategy": "Unit tests with known EPS and price values. Verify calculations match PRD examples (e.g., HOOD). Test configurable multiples.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Develop Manual Verification Support Module",
        "description": "Provide tools and output format to support manual verification of suspicious signals using financial statements and alternative data.",
        "details": "Implement checklist steps: compare trailing EPS with actual financial statements, verify forward EPS with analyst consensus, check recent stock splits, validate earnings growth realism, cross-reference Bloomberg/FactSet data. Output comparison tables showing actual vs expected EPS, implied growth rates, data source mismatches, and verification status icons (✅/⚠️/❌).",
        "testStrategy": "Create mock data sets for manual verification scenarios. Validate output formatting and correctness. Test CLI integration for manual verification mode.",
        "priority": "medium",
        "dependencies": [
          2,
          6,
          7
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Implement Portfolio Ranking Algorithm",
        "description": "Rank portfolio positions by compression magnitude and generate buy/sell/hold signals with confidence levels.",
        "details": "Use compression_pct and data quality flags to rank stocks descending by absolute compression magnitude. Assign signals: BUY for positive compression > threshold, SELL for negative compression < -threshold, HOLD otherwise. Calculate confidence based on data quality and compression extremity. Integrate with portfolio loader and analysis modules.",
        "testStrategy": "Test ranking with sample portfolios. Verify signal assignments and confidence levels match expected logic. Confirm sorting correctness.",
        "priority": "medium",
        "dependencies": [
          4,
          7,
          8
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 11,
        "title": "Develop Markdown Report Generator",
        "description": "Create a report generator producing summary and detailed markdown reports with analysis results, warnings, and recommendations.",
        "details": "Generate summary report with immediate actions (buy/sell), warnings, and portfolio stats. Detailed report includes ticker, company name, prices, P/E ratios, compression, fair values, data quality indicators, signals, confidence, and manual verification status. Use rich and tabulate libraries for formatting. Support output to file paths specified in CLI.",
        "testStrategy": "Generate reports for ISA and SIPP portfolios. Validate markdown formatting and content completeness. Compare output against PRD examples.",
        "priority": "medium",
        "dependencies": [
          10,
          9
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "Implement Command-Line Interface (CLI)",
        "description": "Build CLI commands for portfolio analysis, manual verification, and report export with configuration support.",
        "details": "Use argparse or click to implement commands: 'analyze' with --portfolio and --all options, 'verify' with --ticker option, and --output for report export. Support config.yaml for parameters like data source, cache TTL, thresholds, and scenarios. Provide user-friendly messages and error handling. Integrate with core modules for data fetching, analysis, and reporting.",
        "testStrategy": "Test CLI commands with various options. Validate correct execution paths and error handling. Confirm config file overrides defaults.",
        "priority": "medium",
        "dependencies": [
          2,
          3,
          11,
          9
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 13,
        "title": "Integrate Momentum_Squared Portfolio Format and diet103 Hooks",
        "description": "Ensure compatibility with Momentum_Squared CSV format and implement diet103 hooks for validation and synchronization.",
        "details": "Support import of master portfolio files matching Momentum_Squared format. Implement diet103 hooks: Pre-Analysis Validator (portfolio format), Data Quality Guardian (enforce checks), Portfolio Sync Validator (prevent master file drift), Results Validator (report accuracy). Use hooks to enforce data integrity and integration consistency.",
        "testStrategy": "Test import of master portfolio files. Validate hooks trigger correctly and enforce rules. Cross-check results with Momentum_Squared analysis outputs.",
        "priority": "medium",
        "dependencies": [
          3,
          7,
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "Develop Comprehensive Unit and Integration Test Suite",
        "description": "Create tests covering all modules including calculations, data corrections, validations, API integration, and CLI commands.",
        "details": "Write unit tests for compression calculations, UK stock corrections, stock split detection, fair value scenarios, and data validation rules. Develop integration tests for end-to-end portfolio analysis, Yahoo Finance API integration, report generation, and CLI execution. Use pytest and pytest-cov to ensure 80%+ coverage. Include edge case tests for missing data, outliers, and duplicates.",
        "testStrategy": "Run full test suite with coverage reports. Verify all critical paths and edge cases are tested. Fix any failing tests before release.",
        "priority": "high",
        "dependencies": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "Implement Performance Optimization and Analysis Speed Targets",
        "description": "Optimize data fetching, calculations, and reporting to analyze 20+ stocks per portfolio in under 2 minutes.",
        "details": "Use asynchronous calls or batch requests for Yahoo Finance data fetching to reduce latency. Cache data with TTL to avoid redundant calls. Optimize pandas and numpy operations for vectorized calculations. Profile code to identify bottlenecks. Ensure report generation is efficient. Monitor memory usage and handle large portfolios gracefully.",
        "testStrategy": "Benchmark analysis time on sample portfolios (ISA with 17 positions). Confirm total runtime under 2 minutes. Profile CPU and memory usage. Optimize as needed.",
        "priority": "high",
        "dependencies": [
          2,
          4,
          11,
          12
        ],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-11-29T07:50:54.503Z",
      "updated": "2025-11-29T07:50:54.503Z",
      "description": "Tasks for master context"
    }
  }
}